{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install --quiet \"langchain\" \"langchain-community\" \"beautifulsoup4\" \"requests\" \"jinja2\" \"weasyprint\"\n",
    "!pip install -U --quiet langchain-huggingface\n",
    "!pip install -q gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T16:56:11.991875Z",
     "iopub.status.busy": "2025-10-01T16:56:11.991632Z",
     "iopub.status.idle": "2025-10-01T16:59:03.215136Z",
     "shell.execute_reply": "2025-10-01T16:59:03.214405Z",
     "shell.execute_reply.started": "2025-10-01T16:56:11.991852Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.8/43.8 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m45.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m91.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m779.2/779.2 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m93.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m102.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m104.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.6/302.6 kB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.8/119.8 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m78.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m401.7/401.7 kB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.1/199.1 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m72.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m56.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m41.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.1/168.1 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m86.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "langchain-huggingface 0.3.1 requires huggingface-hub>=0.33.4, but you have huggingface-hub 0.23.2 which is incompatible.\n",
      "datasets 3.6.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.5.1 which is incompatible.\n",
      "datasets 3.6.0 requires huggingface-hub>=0.24.0, but you have huggingface-hub 0.23.2 which is incompatible.\n",
      "cesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
      "tsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.13.1 which is incompatible.\n",
      "dopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\n",
      "gradio 5.31.0 requires huggingface-hub>=0.28.1, but you have huggingface-hub 0.23.2 which is incompatible.\n",
      "plotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install --quiet \\\n",
    "    \"numpy==1.26.4\" \\\n",
    "    \"scipy==1.13.1\" \\\n",
    "    \"scikit-learn==1.5.0\" \\\n",
    "    \"torch==2.3.1\" \\\n",
    "    \"torchaudio==2.3.1\" \\\n",
    "    \"torchvision==0.18.1\" \\\n",
    "    \"transformers==4.41.2\" \\\n",
    "    \"accelerate==0.30.1\" \\\n",
    "    \"bitsandbytes==0.43.1\" \\\n",
    "    \"diffusers==0.27.2\" \\\n",
    "    \"huggingface_hub==0.23.2\" \\\n",
    "    \"peft==0.10.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-10-01T16:59:03.216298Z",
     "iopub.status.busy": "2025-10-01T16:59:03.216101Z",
     "iopub.status.idle": "2025-10-01T16:59:27.152930Z",
     "shell.execute_reply": "2025-10-01T16:59:27.152340Z",
     "shell.execute_reply.started": "2025-10-01T16:59:03.216274Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-01 16:59:06.948226: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1759337947.149897      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1759337947.206947      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "\n",
    "import torch\n",
    "# import torch_xla\n",
    "# import torch_xla.core.xla_model as xm\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from diffusers import DiffusionPipeline\n",
    "\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "from jinja2 import Environment\n",
    "from weasyprint import HTML\n",
    "\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T16:59:27.154127Z",
     "iopub.status.busy": "2025-10-01T16:59:27.153606Z",
     "iopub.status.idle": "2025-10-01T16:59:27.159159Z",
     "shell.execute_reply": "2025-10-01T16:59:27.158400Z",
     "shell.execute_reply.started": "2025-10-01T16:59:27.154108Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "HTML_TEMPLATE_STRING = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html lang=\"en\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "    <title>{{ title }}</title>\n",
    "    <style>\n",
    "        body {\n",
    "            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Helvetica, Arial, sans-serif;\n",
    "            line-height: 1.6;\n",
    "            color: #333;\n",
    "            max-width: 800px;\n",
    "            margin: 20px auto;\n",
    "            padding: 0 20px;\n",
    "            background-color: #f9f9f9;\n",
    "        }\n",
    "        .container {\n",
    "            background-color: #ffffff;\n",
    "            border: 1px solid #ddd;\n",
    "            border-radius: 8px;\n",
    "            padding: 40px;\n",
    "            box-shadow: 0 4px 8px rgba(0,0,0,0.05);\n",
    "        }\n",
    "        h1 {\n",
    "            color: #1a1a1a;\n",
    "            text-align: center;\n",
    "            font-size: 2.5em;\n",
    "            margin-bottom: 0.5em;\n",
    "        }\n",
    "        img.header-image {\n",
    "            max-width: 100%;\n",
    "            height: auto;\n",
    "            border-radius: 8px;\n",
    "            margin-bottom: 20px;\n",
    "            display: block;\n",
    "        }\n",
    "        .divider {\n",
    "            border: 0;\n",
    "            height: 1px;\n",
    "            background: #e0e0e0;\n",
    "            margin: 30px 0;\n",
    "        }\n",
    "        .content-columns {\n",
    "            display: flex;\n",
    "            gap: 20px; /* Space between the columns and the vertical line */\n",
    "        }\n",
    "        .column {\n",
    "            flex: 1; /* This makes both columns take up equal space */\n",
    "            min-width: 0; /* Prevents flexbox overflow */\n",
    "        }\n",
    "        .column.left {\n",
    "            padding-right: 20px;\n",
    "            border-right: 1px solid #e0e0e0; /* The vertical line */\n",
    "        }\n",
    "        p {\n",
    "            font-size: 1em;\n",
    "            text-align: justify;\n",
    "            /* This preserves newlines from the summary text, a cleaner way than replacing \\\\n */\n",
    "            white-space: pre-wrap; \n",
    "        }\n",
    "        .footer {\n",
    "            text-align: center;\n",
    "            margin-top: 40px;\n",
    "            font-size: 0.9em;\n",
    "            color: #888;\n",
    "        }\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "    <div class=\"container\">\n",
    "        <h1>{{ title }}</h1>\n",
    "        <img src=\"{{ image_file }}\" alt=\"{{ title }} Header Image\" class=\"header-image\">\n",
    "        <hr class=\"divider\">\n",
    "        <div class=\"content-columns\">\n",
    "            <div class=\"column left\">\n",
    "                <p>{{ summary_left }}</p>\n",
    "            </div>\n",
    "            <div class=\"column right\">\n",
    "                <p>{{ summary_right }}</p>\n",
    "            </div>\n",
    "        </div>\n",
    "    </div>\n",
    "    <div class=\"footer\">\n",
    "        <p>Generated by AI Agent</p>\n",
    "    </div>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T16:59:35.497617Z",
     "iopub.status.busy": "2025-10-01T16:59:35.497323Z",
     "iopub.status.idle": "2025-10-01T16:59:35.578436Z",
     "shell.execute_reply": "2025-10-01T16:59:35.577659Z",
     "shell.execute_reply.started": "2025-10-01T16:59:35.497596Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are now using= cuda\n"
     ]
    }
   ],
   "source": [
    "def clear_memory():\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "# device = xm.xla_device()\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"We are now using= {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T16:59:36.326377Z",
     "iopub.status.busy": "2025-10-01T16:59:36.325902Z",
     "iopub.status.idle": "2025-10-01T16:59:36.461374Z",
     "shell.execute_reply": "2025-10-01T16:59:36.460674Z",
     "shell.execute_reply.started": "2025-10-01T16:59:36.326352Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "path_qwen = \"/kaggle/input/qwen-3/transformers/14b-base/1\"\n",
    "path_sd = \"/kaggle/input/stable-diffusion-xl/pytorch/base-1-0/1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T16:59:39.259328Z",
     "iopub.status.busy": "2025-10-01T16:59:39.259094Z",
     "iopub.status.idle": "2025-10-01T16:59:39.265127Z",
     "shell.execute_reply": "2025-10-01T16:59:39.264332Z",
     "shell.execute_reply.started": "2025-10-01T16:59:39.259311Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def scrape_from_web(urls = list, file_path = \"content.txt\"):\n",
    "    print(\"Step 1 initialized, Scraping content from the links\")\n",
    "\n",
    "    all_content = []\n",
    "\n",
    "    for url in urls:\n",
    "        try:\n",
    "            loader = WebBaseLoader(url)\n",
    "            docs = loader.load()\n",
    "    \n",
    "            content = \"\\n\".join([doc.page_content for doc in docs])\n",
    "            all_content.append(content)\n",
    "            print(f\"Successfully Scraped content from = {url}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error scraping URL= {e}\")\n",
    "            continue\n",
    "\n",
    "    final_content = \"\\n\\n ++++ARTICLE SEPARATOR++++ \\n\\n\".join(all_content)\n",
    "\n",
    "    try:\n",
    "        with open(file_path, 'w', encoding=\"utf-8\") as f:\n",
    "                f.write(final_content)\n",
    "    \n",
    "        print(f\"Content saved to {file_path}\")\n",
    "        return final_content\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error writing to file: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T17:03:59.736380Z",
     "iopub.status.busy": "2025-10-01T17:03:59.735868Z",
     "iopub.status.idle": "2025-10-01T17:03:59.743557Z",
     "shell.execute_reply": "2025-10-01T17:03:59.742886Z",
     "shell.execute_reply.started": "2025-10-01T17:03:59.736359Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def summarize(content_path = \"content.txt\", summary_path = \"summary.txt\", title_path = \"title.txt\", image_prompt_path = \"image_prompt.txt\"):\n",
    "    print(\"\\nStep 2 initialized, Summarizing content\")\n",
    "\n",
    "    model = None\n",
    "    tokenizer = None\n",
    "    pipe = None\n",
    "    llm = None\n",
    "    \n",
    "    try:\n",
    "        with open(content_path, 'r', encoding=\"utf-8\") as f:\n",
    "            content = f.read()\n",
    "\n",
    "        tokenizer = AutoTokenizer.from_pretrained(path_qwen,\n",
    "                                                  local_files_only=True,\n",
    "                                                 )\n",
    "        model = AutoModelForCausalLM.from_pretrained(path_qwen,\n",
    "                                                      local_files_only=True,\n",
    "                                                      device_map=\"auto\",\n",
    "                                                      torch_dtype=\"auto\"\n",
    "                                                     )\n",
    "        \n",
    "        pipe = pipeline(\"text-generation\", \n",
    "                        model = model, \n",
    "                        tokenizer = tokenizer,\n",
    "                        max_new_tokens = 1024,\n",
    "                        do_sample = True,\n",
    "                        temperature = 0.8,\n",
    "                        top_p = 0.95\n",
    "                       )\n",
    "        \n",
    "        # llm = HuggingFacePipeline(pipeline = pipe)\n",
    "\n",
    "        # text_splitter = RecursiveCharacterTextSplitter(chunk_size = 1024,\n",
    "        #                                                chunk_overlap = 100\n",
    "        #                                               )\n",
    "        # split_docs = text_splitter.create_documents([content])\n",
    "\n",
    "        # refine_template = (\n",
    "        #     \"Your job is to produce a final summary\\n\"\n",
    "        #     \"We have provided an existing summary up to a certain point: {existing_answer}\\n\"\n",
    "        #     \"We have the opportunity to refine the existing summary\"\n",
    "        #     \"(only if needed) with some more context below.\\n\"\n",
    "        #     \"------------\\n\"\n",
    "        #     \"{text}\\n\"\n",
    "        #     \"------------\\n\"\n",
    "        #     \"Given the new context, refine the original summary into a final, high-quality newsletter summary.\"\n",
    "        # )\n",
    "        # refine_prompt = PromptTemplate.from_template(refine_template)\n",
    "\n",
    "        # question_template = \"Summarize the key points of this text for a newsletter:\\n\\n{text}\"\n",
    "        # question_prompt = PromptTemplate.from_template(question_template)\n",
    "\n",
    "        # summary_chain = load_summarize_chain(llm = llm, \n",
    "        #                                      chain_type = \"refine\",\n",
    "        #                                      question_prompt = question_prompt,\n",
    "        #                                      refine_prompt = refine_prompt,\n",
    "        #                                      return_intermediate_steps = False\n",
    "        #                                     )\n",
    "        \n",
    "        # summary_clean = summary_chain.invoke(split_docs)['output_text'].strip()\n",
    "\n",
    "        # Summary part\n",
    "        summary_prompt = f\"Please provide a detailed, well-structured, and comprehensive summary of the following articles. The summary should be written in the style of a professional newsletter. Extract the key insights, main points, and any important conclusions. The final summary should be between 120 and 300 words.\\n\\nARTICLES:\\n\\n{content}\"\n",
    "        summary_output = pipe(summary_prompt)\n",
    "        summary_clean = summary_output[0]['generated_text'][len(summary_prompt):].strip()\n",
    "\n",
    "        with open(summary_path, 'w', encoding=\"utf-8\") as f:\n",
    "            f.write(summary_clean)\n",
    "\n",
    "        print(f\"Full Summary saved to {summary_path}\")\n",
    "\n",
    "        # Title part\n",
    "\n",
    "        print(\"\\nStep 3 initialized, Generating Title\")\n",
    "\n",
    "        title_prompt = f\"Based on the following summary, generate one extremely catchy and concise newsletter title. The title must be under 5 words.\\n\\nSUMMARY:\\n{summary_clean}\"\n",
    "        title_output = pipe(title_prompt, max_new_tokens=20)\n",
    "        final_title = title_output[0]['generated_text'][len(title_prompt):].strip().split('\\n')[0].strip().lstrip('1234567890.-* ')\n",
    "        \n",
    "        with open(title_path, 'w', encoding=\"utf-8\") as f: \n",
    "            f.write(final_title)\n",
    "            \n",
    "        print(f\"Generated title saved to {title_path}\")\n",
    "        \n",
    "        \n",
    "        # Image prompt part\n",
    "\n",
    "        print(\"\\nStep 4 initialized, Generating Image prompt\")\n",
    "\n",
    "        image_gen_prompt = f\"Based on the following text, create a short, visually descriptive prompt for an AI image generator. Focus on concrete objects, scenes, colors, and styles. Do not include abstract concepts. The prompt should be a single, concise sentence of no more than 50 words.\\n\\nTEXT:\\n{summary_clean}\"\n",
    "        image_prompt_output = pipe(image_gen_prompt, max_new_tokens=60)\n",
    "        final_image_prompt = image_prompt_output[0]['generated_text'][len(image_gen_prompt):].strip().replace('\"', '')\n",
    "        \n",
    "        with open(image_prompt_path, \"w\", encoding=\"utf-8\") as f: \n",
    "            f.write(final_image_prompt)\n",
    "            \n",
    "        print(f\"Generated prompt saved to {image_prompt_path}\")\n",
    "\n",
    "        return summary_clean, final_title, final_image_prompt\n",
    "\n",
    "    finally:\n",
    "        print(\"Cleaning up Qwen model from memory\")\n",
    "\n",
    "        if 'model' in locals() and model is not None:\n",
    "            del model\n",
    "        if 'tokenizer' in locals() and tokenizer is not None:\n",
    "            del tokenizer\n",
    "        if 'pipe' in locals() and pipe is not None:\n",
    "            del pipe\n",
    "        if 'llm' in locals() and llm is not None:\n",
    "            del llm\n",
    "        clear_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T17:04:02.989326Z",
     "iopub.status.busy": "2025-10-01T17:04:02.989063Z",
     "iopub.status.idle": "2025-10-01T17:04:02.997019Z",
     "shell.execute_reply": "2025-10-01T17:04:02.996366Z",
     "shell.execute_reply.started": "2025-10-01T17:04:02.989308Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# def generate_title_and_prompt(summary_path = \"summary.txt\", title_path = \"title.txt\", image_prompt_path = \"image_prompt.txt\"):\n",
    "#     print(\"Step 3 initialized. Generating title\")\n",
    "\n",
    "#     gen_model = None\n",
    "#     gen_tokenizer = None\n",
    "#     gen_pipe = None\n",
    "    \n",
    "#     try:\n",
    "# #Title Part\n",
    "#         with open(summary_path, 'r', encoding=\"utf-8\") as f:\n",
    "#             summary_clean = f.read()\n",
    "\n",
    "#         gen_tokenizer = AutoTokenizer.from_pretrained(path_phi3,\n",
    "#                                                       local_files_only=True,\n",
    "#                                                       trust_remote_code=True)\n",
    "#         gen_model = AutoModelForCausalLM.from_pretrained(path_phi3,\n",
    "#                                                          local_files_only=True,\n",
    "#                                                          trust_remote_code=True, \n",
    "#                                                          device_map=\"auto\", \n",
    "#                                                          torch_dtype=\"auto\", \n",
    "#                                                          load_in_8bit=(device == \"cuda\")\n",
    "#                                                         )\n",
    "#         gen_pipe = pipeline(\"text-generation\", \n",
    "#                         model = gen_model, \n",
    "#                         tokenizer = gen_tokenizer,\n",
    "#                         max_new_tokens = 80,\n",
    "#                         eos_token_id = gen_tokenizer.eos_token_id\n",
    "#                        )\n",
    "\n",
    "#         title_messages = [\n",
    "#             {\"role\": \"user\", \"content\": f\"Based on this summary, generate only one extremely catchy but concise newsletter title. The title must be under 12 words.\\n\\nSUMMARY:\\n{summary_clean}\"}\n",
    "#         ]\n",
    "\n",
    "#         title_outputs = gen_pipe(title_messages, max_new_tokens = 30)\n",
    "\n",
    "#         generated_title = title_outputs[0]['generated_text'][-1]['content']\n",
    "#         final_title = generated_title.strip().split('\\n')[0].strip().lstrip('1234567890.-* ')\n",
    "        \n",
    "#         with open(title_path, 'w', encoding=\"utf-8\") as f:\n",
    "#             f.write(final_title)\n",
    "\n",
    "#         print(f\"title saved to {title_path}\")\n",
    "\n",
    "# #Image prompt Part        \n",
    "#         print(\"Step 4 initialized. Generating image prompt\")\n",
    "\n",
    "#         prompt_messages = [{\"role\": \"user\", \"content\": f\"Based on the following text, create a short, visually descriptive prompt for an AI image generator. Focus on concrete objects, scenes, colors, and styles. Do not include abstract concepts. Maximum 60 words.\\n\\nTEXT:\\n{summary_clean}\"}]\n",
    "#         prompt_outputs = gen_pipe(prompt_messages, max_new_tokens=80)\n",
    "#         generated_prompt = prompt_outputs[0]['generated_text'][-1]['content']\n",
    "#         final_image_prompt = generated_prompt.strip().replace('\"', '')\n",
    "        \n",
    "#         with open(image_prompt_path, \"w\", encoding=\"utf-8\") as f:\n",
    "#             f.write(final_image_prompt) \n",
    "\n",
    "#         print(f\"image prompt saved to {image_prompt_path}\")\n",
    "\n",
    "#         return final_title, final_image_prompt\n",
    "#     finally:\n",
    "#         print(\"Cleaning up title gen model from memory\")\n",
    "#         del gen_model\n",
    "#         del gen_tokenizer\n",
    "#         del gen_pipe\n",
    "#         clear_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T17:04:03.112566Z",
     "iopub.status.busy": "2025-10-01T17:04:03.111913Z",
     "iopub.status.idle": "2025-10-01T17:04:03.117541Z",
     "shell.execute_reply": "2025-10-01T17:04:03.116882Z",
     "shell.execute_reply.started": "2025-10-01T17:04:03.112543Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def generate_newsletter_image(image_prompt_path = \"image_prompt.txt\", title_path = \"title.txt\", image_path = \"newsletter_image.png\"):\n",
    "    print(\"\\nStep 5 initialized, Generating image for newsletter\")\n",
    "\n",
    "    pipe = None\n",
    "    \n",
    "    try:\n",
    "        with open(image_prompt_path, 'r', encoding=\"utf-8\") as f:\n",
    "            visual_prompt = f.read().strip()\n",
    "        with open(title_path, 'r', encoding=\"utf-8\") as f:\n",
    "            title = f.read().strip()\n",
    "\n",
    "        final_prompt = f\"Image for a newsletter titled '{title}'. A clean, professional illustration of: {visual_prompt}. Marketing aesthetic, vibrant colors.\"\n",
    "        \n",
    "        pipe = DiffusionPipeline.from_pretrained(\n",
    "            path_sd,\n",
    "            torch_dtype = torch.float16,\n",
    "            variant = \"fp16\",\n",
    "            use_safetensors = True\n",
    "        )\n",
    "\n",
    "        pipe = pipe.to(device)\n",
    "\n",
    "        image = pipe(prompt = final_prompt).images[0]\n",
    "        image.save(image_path)\n",
    "        print(f\"Image saved to {image_path}\")\n",
    "\n",
    "        return image_path\n",
    "\n",
    "    finally:\n",
    "        print(\"Cleaning up diffusion model from memory\")\n",
    "        \n",
    "        if 'pipe' in locals() and pipe is not None:\n",
    "            del pipe\n",
    "\n",
    "        clear_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T17:04:03.249023Z",
     "iopub.status.busy": "2025-10-01T17:04:03.248837Z",
     "iopub.status.idle": "2025-10-01T17:04:03.254830Z",
     "shell.execute_reply": "2025-10-01T17:04:03.254021Z",
     "shell.execute_reply.started": "2025-10-01T17:04:03.249009Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def assemble(summary_path, image_path, title_path, output_name=\"newsletter\"):\n",
    "    print(\"Step 6 initialized, Assembling final Newsletter\")\n",
    "\n",
    "    try:\n",
    "        with open(summary_path, 'r', encoding=\"utf-8\") as f:\n",
    "            summary_text = f.read()\n",
    "        with open(title_path, 'r', encoding=\"utf-8\") as f:\n",
    "            title = f.read().strip()\n",
    "\n",
    "        mid = len(summary_text) // 2\n",
    "        split_point = summary_text.find(' ', mid)\n",
    "\n",
    "        if split_point == -1:\n",
    "            split_point = mid\n",
    "\n",
    "        summary_left = summary_text[:split_point]\n",
    "        summary_right = summary_text[split_point:].strip()\n",
    "\n",
    "        template = Environment().from_string(HTML_TEMPLATE_STRING)\n",
    "\n",
    "        html_content = template.render(\n",
    "            title = title,\n",
    "            summary_left = summary_left,\n",
    "            summary_right = summary_right,\n",
    "            image_file = image_path\n",
    "        )\n",
    "\n",
    "        html_file_path = f\"{output_name}.html\"\n",
    "\n",
    "        with open(html_file_path, 'w', encoding=\"utf-8\") as f:\n",
    "            f.write(html_content)\n",
    "\n",
    "        print(f\"HTML Newsletter saved to {html_file_path}\")\n",
    "\n",
    "        return html_file_path\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occured during assembly= {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T17:04:03.390702Z",
     "iopub.status.busy": "2025-10-01T17:04:03.390270Z",
     "iopub.status.idle": "2025-10-01T17:04:03.395982Z",
     "shell.execute_reply": "2025-10-01T17:04:03.395267Z",
     "shell.execute_reply.started": "2025-10-01T17:04:03.390682Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def run_full_pipeline(urls_string):\n",
    "    print(\"Starting automated Newsletter Generation\")\n",
    "\n",
    "    urls = [url.strip() for url in urls_string.split(',') if url.strip()]\n",
    "    if not urls:\n",
    "        yield \"Please upload atleast one url\", None\n",
    "        return\n",
    "    \n",
    "    content_file = \"content.txt\"\n",
    "    summary_file = \"summary.txt\"\n",
    "    title_file = \"title.txt\"\n",
    "    image_prompt_file = \"image_prompt.txt\"\n",
    "    image_file = \"newsletter_image.png\"\n",
    "\n",
    "    log = \"\"\n",
    "    try:\n",
    "        log += \"Step 1/4= Scraping content from URLs+++++\\n\"\n",
    "        yield log, None\n",
    "        scrape_from_web(urls, content_file)\n",
    "        log += \"Step 1/4= Scraping Completed\\n\"\n",
    "        yield log, None\n",
    "\n",
    "        log += \"Step 2/4= Summarizing, Generating Title+Image Prompt Content+++++\\n\"\n",
    "        yield log, None\n",
    "        summarize(content_file, summary_file, title_file, image_prompt_file)\n",
    "        log += \"Step 2/4= Summarizing, Generating Title+Image Prompt Complete\\n\"\n",
    "        yield log, None\n",
    "\n",
    "        # yield \"Step 3/5= Generating Title and Image_prompt+++++\", None\n",
    "        # generate_title_and_prompt(summary_file, title_file, image_prompt_file)\n",
    "        # yield \"Step 3/5= Title and Image_prompt Generated\", None\n",
    "        \n",
    "        log += \"Step 3/4= Generating Image++++\\n\"\n",
    "        yield log, None\n",
    "        image_file_path = generate_newsletter_image(image_prompt_file, title_file, image_file)\n",
    "        log += \"Step 3/4= Image Generated\\n\"\n",
    "        yield log, None\n",
    "\n",
    "        \n",
    "        log += \"Step 4/4= Assembling final Newsletter+++++\\n\"\n",
    "        yield log, None\n",
    "        html_file_path = assemble(summary_file, image_file_path, title_file)\n",
    "        log += \"Step 4/4= Newsletter Assembled\\n\"\n",
    "        yield log, None\n",
    "\n",
    "\n",
    "        log += \"Pipeline finished!!! Your newsletter is ready for download.\"    \n",
    "\n",
    "        downloadables = [content_file, summary_file, title_file, image_prompt_file, image_file_path, html_file_path]\n",
    "\n",
    "        yield log, downloadables\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_message = f\"Error occurred = {str(e)}\"\n",
    "        print(error_message)\n",
    "        yield error_message, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T17:04:03.537898Z",
     "iopub.status.busy": "2025-10-01T17:04:03.537731Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* Running on public URL: https://83b3b258ca684063a8.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://83b3b258ca684063a8.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting automated Newsletter Generation\n",
      "Step 1 initialized, Scraping content from the links\n",
      "Successfully Scraped content from = https://www.theverge.com/24172762/sonos-ace-headphones-video-review\n",
      "Content saved to content.txt\n",
      "\n",
      "Step 2 initialized, Summarizing content\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at /kaggle/input/pegasus-daily-local/hub/google--pegasus-cnn_dailymail and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Your max_length is set to 512, but your input_length is only 217. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=108)\n",
      "Your max_length is set to 512, but your input_length is only 458. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=229)\n",
      "Your max_length is set to 512, but your input_length is only 472. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=236)\n",
      "Your max_length is set to 512, but your input_length is only 455. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=227)\n",
      "Your max_length is set to 512, but your input_length is only 447. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=223)\n",
      "Your max_length is set to 512, but your input_length is only 438. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=219)\n",
      "Your max_length is set to 512, but your input_length is only 460. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=230)\n",
      "Your max_length is set to 512, but your input_length is only 290. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=145)\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Summary saved to summary.txt\n",
      "Cleaning up summarization model from memory\n",
      "Step 3 initialized. Generating title\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8030ff22bf8f4f85b66ebd3585dd4ed8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title saved to title.txt\n",
      "Step 4 initialized. Generating image prompt\n",
      "image prompt saved to image_prompt.txt\n",
      "Cleaning up title gen model from memory\n",
      "\n",
      "Step 5 initialized, Generating image for newsletter\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69bd272553fd4d86a70291b503715ed2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (94 > 77). Running this sequence through the model will result in indexing errors\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [\"google's ai search restrictions and vox media's copyright.. marketing aesthetic, vibrant colors.\"]\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (94 > 77). Running this sequence through the model will result in indexing errors\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [\"google's ai search restrictions and vox media's copyright.. marketing aesthetic, vibrant colors.\"]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d227a18ac9724a599c1a78f524d1edb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image saved to newsletter_image.png\n",
      "Cleaning up diffusion model from memory\n",
      "Step 4 initialized, Assembling final Newsletter\n",
      "HTML Newsletter saved to newsletter.html\n"
     ]
    }
   ],
   "source": [
    "with gr.Blocks(theme='monochrome', title=\"Agentic Newsletter Generator\") as demo:\n",
    "    gr.Markdown('''<div style=\"position: absolute; top: 20px; right: 20px; opacity: 0.9; z-index: -1;\">\n",
    "  <img src=\"https://ibb.co/jkfszwYT\" alt=\"Watermark\" width=\"200\">\n",
    "</div>\n",
    "\n",
    "<center>\n",
    "<h1>\n",
    "Agentic Newsletter Generator\n",
    "</h1>\n",
    "</center>\n",
    "\n",
    "- Enter one or more URLs (comma+separated)\n",
    "- The agent will automatically parse, summarize, and newsletter will be created\n",
    "---------\n",
    "''')\n",
    "    \n",
    "    with gr.Row():\n",
    "        with gr.Column(scale = 2):\n",
    "            url_input = gr.Textbox(label=\"Enter URLs Here\", placeholder=\"https://www.<websitename>.<domain>/\")\n",
    "            generate_button = gr.Button(\"Generate Newsletter\", variant=\"primary\")\n",
    "\n",
    "        with gr.Column(scale = 1):\n",
    "            status_output = gr.Textbox(label=\"Agent's progress\", interactive=False, lines=8)\n",
    "            file_output = gr.File(label=\"Downloadables output\")\n",
    "\n",
    "    generate_button.click(\n",
    "        fn = run_full_pipeline,\n",
    "        inputs = url_input,\n",
    "        outputs=[status_output, file_output]\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch(debug=True, share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "isSourceIdPinned": false,
     "modelId": 972,
     "modelInstanceId": 3321,
     "sourceId": 4529,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 322000,
     "modelInstanceId": 301529,
     "sourceId": 363153,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
