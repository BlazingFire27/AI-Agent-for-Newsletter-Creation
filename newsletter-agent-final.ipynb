{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":4529,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":3321,"modelId":972},{"sourceId":69160,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":57690,"modelId":79606}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install --quiet \"langchain\" \"langchain-community\" \"beautifulsoup4\" \"requests\" \"jinja2\" \"weasyprint\"\n!pip install -U --quiet langchain-huggingface\n!pip install -q gradio","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-02T06:18:11.417445Z","iopub.execute_input":"2025-10-02T06:18:11.418148Z","iopub.status.idle":"2025-10-02T06:18:30.357259Z","shell.execute_reply.started":"2025-10-02T06:18:11.418121Z","shell.execute_reply":"2025-10-02T06:18:30.356474Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m40.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m449.5/449.5 kB\u001b[0m \u001b[31m34.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.6/48.6 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m78.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m82.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m850.6/850.6 kB\u001b[0m \u001b[31m48.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ndatasets 3.6.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.5.1 which is incompatible.\ngoogle-colab 1.0.0 requires google-auth==2.38.0, but you have google-auth 2.40.3 which is incompatible.\ngoogle-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\ngoogle-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\ngoogle-colab 1.0.0 requires requests==2.32.3, but you have requests 2.32.5 which is incompatible.\ngoogle-colab 1.0.0 requires tornado==6.4.2, but you have tornado 6.5.1 which is incompatible.\npandas-gbq 0.29.1 requires google-api-core<3.0.0,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.5.1 which is incompatible.\ngoogle-cloud-storage 2.19.0 requires google-api-core<3.0.0dev,>=2.15.0, but you have google-api-core 1.34.1 which is incompatible.\ndataproc-spark-connect 0.7.5 requires google-api-core>=2.19, but you have google-api-core 1.34.1 which is incompatible.\nbigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\njupyter-kernel-gateway 2.5.2 requires jupyter-client<8.0,>=5.2.0, but you have jupyter-client 8.6.3 which is incompatible.\u001b[0m\u001b[31m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m564.3/564.3 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndatasets 3.6.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.5.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install --quiet \\\n    \"numpy==1.26.4\" \\\n    \"scipy==1.13.1\" \\\n    \"scikit-learn==1.5.0\" \\\n    \"torch==2.3.1\" \\\n    \"torchaudio==2.3.1\" \\\n    \"torchvision==0.18.1\" \\\n    \"transformers==4.41.2\" \\\n    \"accelerate==0.30.1\" \\\n    \"bitsandbytes==0.43.1\" \\\n    \"diffusers==0.27.2\" \\\n    \"huggingface_hub==0.23.2\" \\\n    \"peft==0.10.0\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-02T06:18:30.358586Z","iopub.execute_input":"2025-10-02T06:18:30.358776Z","iopub.status.idle":"2025-10-02T06:21:21.323645Z","shell.execute_reply.started":"2025-10-02T06:18:30.358756Z","shell.execute_reply":"2025-10-02T06:21:21.322990Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.8/43.8 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m43.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m87.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m779.2/779.2 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m89.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m107.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m106.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.6/302.6 kB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.8/119.8 MB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m66.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m401.7/401.7 kB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.1/199.1 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m92.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m69.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m47.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.1/168.1 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m93.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nlangchain-huggingface 0.3.1 requires huggingface-hub>=0.33.4, but you have huggingface-hub 0.23.2 which is incompatible.\ndatasets 3.6.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.5.1 which is incompatible.\ndatasets 3.6.0 requires huggingface-hub>=0.24.0, but you have huggingface-hub 0.23.2 which is incompatible.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ntsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.13.1 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\ngradio 5.31.0 requires huggingface-hub>=0.28.1, but you have huggingface-hub 0.23.2 which is incompatible.\nplotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import os\nimport gc\n\nimport torch\n# import torch_xla\n# import torch_xla.core.xla_model as xm\n\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\nfrom diffusers import DiffusionPipeline\n\nfrom langchain_community.document_loaders import WebBaseLoader\nfrom langchain.chains.summarize import load_summarize_chain\nfrom langchain_huggingface import HuggingFacePipeline\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\nfrom langchain.prompts import PromptTemplate\nfrom langchain_core.documents import Document\n\nfrom jinja2 import Environment\nfrom weasyprint import HTML\n\nimport gradio as gr","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-02T06:21:21.324819Z","iopub.execute_input":"2025-10-02T06:21:21.325114Z","iopub.status.idle":"2025-10-02T06:21:45.860337Z","shell.execute_reply.started":"2025-10-02T06:21:21.325077Z","shell.execute_reply":"2025-10-02T06:21:45.859748Z"}},"outputs":[{"name":"stderr","text":"2025-10-02 06:21:25.004387: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1759386085.199155      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1759386085.254856      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"HTML_TEMPLATE_STRING = \"\"\"\n<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n    <meta charset=\"UTF-8\">\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    <title>{{ title }}</title>\n    <style>\n        body {\n            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Helvetica, Arial, sans-serif;\n            line-height: 1.6;\n            color: #333;\n            max-width: 800px;\n            margin: 20px auto;\n            padding: 0 20px;\n            background-color: #f9f9f9;\n        }\n        .container {\n            background-color: #ffffff;\n            border: 1px solid #ddd;\n            border-radius: 8px;\n            padding: 40px;\n            box-shadow: 0 4px 8px rgba(0,0,0,0.05);\n        }\n        h1 {\n            color: #1a1a1a;\n            text-align: center;\n            font-size: 2.5em;\n            margin-bottom: 0.5em;\n        }\n        img.header-image {\n            max-width: 100%;\n            height: auto;\n            border-radius: 8px;\n            margin-bottom: 20px;\n            display: block;\n        }\n        .divider {\n            border: 0;\n            height: 1px;\n            background: #e0e0e0;\n            margin: 30px 0;\n        }\n        .content-columns {\n            display: flex;\n            gap: 20px; /* Space between the columns and the vertical line */\n        }\n        .column {\n            flex: 1; /* This makes both columns take up equal space */\n            min-width: 0; /* Prevents flexbox overflow */\n        }\n        .column.left {\n            padding-right: 20px;\n            border-right: 1px solid #e0e0e0; /* The vertical line */\n        }\n        p {\n            font-size: 1em;\n            text-align: justify;\n            /* This preserves newlines from the summary text, a cleaner way than replacing \\\\n */\n            white-space: pre-wrap; \n        }\n        .footer {\n            text-align: center;\n            margin-top: 40px;\n            font-size: 0.9em;\n            color: #888;\n        }\n    </style>\n</head>\n<body>\n    <div class=\"container\">\n        <h1>{{ title }}</h1>\n        <img src=\"{{ image_file }}\" alt=\"{{ title }} Header Image\" class=\"header-image\">\n        <hr class=\"divider\">\n        <div class=\"content-columns\">\n            <div class=\"column left\">\n                <p>{{ summary_left }}</p>\n            </div>\n            <div class=\"column right\">\n                <p>{{ summary_right }}</p>\n            </div>\n        </div>\n    </div>\n    <div class=\"footer\">\n        <p>Generated by AI Agent</p>\n    </div>\n</body>\n</html>\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-02T06:21:52.350486Z","iopub.execute_input":"2025-10-02T06:21:52.351396Z","iopub.status.idle":"2025-10-02T06:21:52.356286Z","shell.execute_reply.started":"2025-10-02T06:21:52.351369Z","shell.execute_reply":"2025-10-02T06:21:52.355572Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"def clear_memory():\n    gc.collect()\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n\n# device = xm.xla_device()\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"We are now using= {device}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-02T06:21:52.477983Z","iopub.execute_input":"2025-10-02T06:21:52.478418Z","iopub.status.idle":"2025-10-02T06:21:52.554339Z","shell.execute_reply.started":"2025-10-02T06:21:52.478400Z","shell.execute_reply":"2025-10-02T06:21:52.553630Z"}},"outputs":[{"name":"stdout","text":"We are now using= cuda\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"path_qwen = \"/kaggle/input/qwen2/transformers/7b-instruct/1\"\npath_sd = \"/kaggle/input/stable-diffusion-xl/pytorch/base-1-0/1\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-02T06:31:10.581286Z","iopub.execute_input":"2025-10-02T06:31:10.581548Z","iopub.status.idle":"2025-10-02T06:31:10.585281Z","shell.execute_reply.started":"2025-10-02T06:31:10.581529Z","shell.execute_reply":"2025-10-02T06:31:10.584528Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"def scrape_from_web(urls = list, file_path = \"content.txt\"):\n    print(\"Step 1 initialized, Scraping content from the links\")\n\n    all_content = []\n\n    for url in urls:\n        try:\n            loader = WebBaseLoader(url)\n            docs = loader.load()\n    \n            content = \"\\n\".join([doc.page_content for doc in docs])\n            all_content.append(content)\n            print(f\"Successfully Scraped content from = {url}\")\n            \n        except Exception as e:\n            print(f\"Error scraping URL= {e}\")\n            continue\n\n    final_content = \"\\n\\n ++++ARTICLE SEPARATOR++++ \\n\\n\".join(all_content)\n\n    try:\n        with open(file_path, 'w', encoding=\"utf-8\") as f:\n                f.write(final_content)\n    \n        print(f\"Content saved to {file_path}\")\n        return final_content\n        \n    except Exception as e:\n        print(f\"Error writing to file: {e}\")\n        return None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-02T06:31:11.555300Z","iopub.execute_input":"2025-10-02T06:31:11.555506Z","iopub.status.idle":"2025-10-02T06:31:11.560882Z","shell.execute_reply.started":"2025-10-02T06:31:11.555492Z","shell.execute_reply":"2025-10-02T06:31:11.560050Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"def summarize(content_path = \"content.txt\", summary_path = \"summary.txt\", title_path = \"title.txt\", image_prompt_path = \"image_prompt.txt\"):\n    print(\"\\nStep 2 initialized, Summarizing content\")\n\n    model = None\n    tokenizer = None\n    pipe = None\n    llm = None\n    \n    try:\n        with open(content_path, 'r', encoding=\"utf-8\") as f:\n            content = f.read()\n\n        tokenizer = AutoTokenizer.from_pretrained(path_qwen,\n                                                  local_files_only=True,\n                                                 )\n        model = AutoModelForCausalLM.from_pretrained(path_qwen,\n                                                      local_files_only=True,\n                                                      device_map=\"auto\",\n                                                      torch_dtype=\"auto\",\n                                                      load_in_8bit=True\n                                                     )\n        \n        pipe = pipeline(\"text-generation\", \n                        model = model, \n                        tokenizer = tokenizer,\n                        max_new_tokens = 1024,\n                        do_sample = True,\n                        temperature = 0.8,\n                        top_p = 0.95\n                       )\n        \n        # llm = HuggingFacePipeline(pipeline = pipe)\n\n        # text_splitter = RecursiveCharacterTextSplitter(chunk_size = 1024,\n        #                                                chunk_overlap = 100\n        #                                               )\n        # split_docs = text_splitter.create_documents([content])\n\n        # refine_template = (\n        #     \"Your job is to produce a final summary\\n\"\n        #     \"We have provided an existing summary up to a certain point: {existing_answer}\\n\"\n        #     \"We have the opportunity to refine the existing summary\"\n        #     \"(only if needed) with some more context below.\\n\"\n        #     \"------------\\n\"\n        #     \"{text}\\n\"\n        #     \"------------\\n\"\n        #     \"Given the new context, refine the original summary into a final, high-quality newsletter summary.\"\n        # )\n        # refine_prompt = PromptTemplate.from_template(refine_template)\n\n        # question_template = \"Summarize the key points of this text for a newsletter:\\n\\n{text}\"\n        # question_prompt = PromptTemplate.from_template(question_template)\n\n        # summary_chain = load_summarize_chain(llm = llm, \n        #                                      chain_type = \"refine\",\n        #                                      question_prompt = question_prompt,\n        #                                      refine_prompt = refine_prompt,\n        #                                      return_intermediate_steps = False\n        #                                     )\n        \n        # summary_clean = summary_chain.invoke(split_docs)['output_text'].strip()\n\n        # Summary part\n        summary_prompt = f\"Please provide a detailed, well-structured, and comprehensive summary of the following articles. The summary should be written in the style of a professional newsletter. Extract the key insights, main points, and any important conclusions. The final summary should be between 120 and 300 words.\\n\\nARTICLES:\\n\\n{content}\"\n        messages = [{\"role\": \"user\", \"content\": summary_prompt}]\n        summary_output = pipe(messages)\n        summary_clean = summary_output[0]['generated_text'][-1]['content'].strip()\n\n        with open(summary_path, 'w', encoding=\"utf-8\") as f:\n            f.write(summary_clean)\n\n        print(f\"Full Summary saved to {summary_path}\")\n\n        # Title part\n\n        print(\"\\nStep 3 initialized, Generating Title\")\n\n        title_prompt = f\"Based on the following summary, generate one extremely catchy and concise newsletter title. The title must be under 5 words.\\n\\nSUMMARY:\\n{summary_clean}\"\n        messages = [{\"role\": \"user\", \"content\": title_prompt}]        \n        title_output = pipe(messages, max_new_tokens=20)\n        final_title = title_output[0]['generated_text'][-1]['content'].strip().split('\\n')[0].strip().lstrip('1234567890.-* ')\n        \n        with open(title_path, 'w', encoding=\"utf-8\") as f: \n            f.write(final_title)\n            \n        print(f\"Generated title saved to {title_path}\")\n        \n        \n        # Image prompt part\n\n        print(\"\\nStep 4 initialized, Generating Image prompt\")\n\n        image_gen_prompt = f\"Based on the following text, create a short, visually descriptive prompt for an AI image generator. Focus on concrete objects, scenes, colors, and styles. Do not include abstract concepts. The prompt should be a single, concise sentence of no more than 50 words.\\n\\nTEXT:\\n{summary_clean}\"\n        messages = [{\"role\": \"user\", \"content\": image_gen_prompt}]\n        image_prompt_output = pipe(messages, max_new_tokens=60)\n        final_image_prompt = image_prompt_output[0]['generated_text'][-1]['content'].strip().replace('\"', '')\n        \n        with open(image_prompt_path, \"w\", encoding=\"utf-8\") as f: \n            f.write(final_image_prompt)\n            \n        print(f\"Generated prompt saved to {image_prompt_path}\")\n\n        return summary_clean, final_title, final_image_prompt\n\n    finally:\n        print(\"Cleaning up Qwen model from memory\")\n\n        if 'model' in locals() and model is not None:\n            del model\n        if 'tokenizer' in locals() and tokenizer is not None:\n            del tokenizer\n        if 'pipe' in locals() and pipe is not None:\n            del pipe\n        if 'llm' in locals() and llm is not None:\n            del llm\n        clear_memory()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-02T06:37:02.793376Z","iopub.execute_input":"2025-10-02T06:37:02.793647Z","iopub.status.idle":"2025-10-02T06:37:02.804254Z","shell.execute_reply.started":"2025-10-02T06:37:02.793626Z","shell.execute_reply":"2025-10-02T06:37:02.803481Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"# def generate_title_and_prompt(summary_path = \"summary.txt\", title_path = \"title.txt\", image_prompt_path = \"image_prompt.txt\"):\n#     print(\"Step 3 initialized. Generating title\")\n\n#     gen_model = None\n#     gen_tokenizer = None\n#     gen_pipe = None\n    \n#     try:\n# #Title Part\n#         with open(summary_path, 'r', encoding=\"utf-8\") as f:\n#             summary_clean = f.read()\n\n#         gen_tokenizer = AutoTokenizer.from_pretrained(path_phi3,\n#                                                       local_files_only=True,\n#                                                       trust_remote_code=True)\n#         gen_model = AutoModelForCausalLM.from_pretrained(path_phi3,\n#                                                          local_files_only=True,\n#                                                          trust_remote_code=True, \n#                                                          device_map=\"auto\", \n#                                                          torch_dtype=\"auto\", \n#                                                          load_in_8bit=(device == \"cuda\")\n#                                                         )\n#         gen_pipe = pipeline(\"text-generation\", \n#                         model = gen_model, \n#                         tokenizer = gen_tokenizer,\n#                         max_new_tokens = 80,\n#                         eos_token_id = gen_tokenizer.eos_token_id\n#                        )\n\n#         title_messages = [\n#             {\"role\": \"user\", \"content\": f\"Based on this summary, generate only one extremely catchy but concise newsletter title. The title must be under 12 words.\\n\\nSUMMARY:\\n{summary_clean}\"}\n#         ]\n\n#         title_outputs = gen_pipe(title_messages, max_new_tokens = 30)\n\n#         generated_title = title_outputs[0]['generated_text'][-1]['content']\n#         final_title = generated_title.strip().split('\\n')[0].strip().lstrip('1234567890.-* ')\n        \n#         with open(title_path, 'w', encoding=\"utf-8\") as f:\n#             f.write(final_title)\n\n#         print(f\"title saved to {title_path}\")\n\n# #Image prompt Part        \n#         print(\"Step 4 initialized. Generating image prompt\")\n\n#         prompt_messages = [{\"role\": \"user\", \"content\": f\"Based on the following text, create a short, visually descriptive prompt for an AI image generator. Focus on concrete objects, scenes, colors, and styles. Do not include abstract concepts. Maximum 60 words.\\n\\nTEXT:\\n{summary_clean}\"}]\n#         prompt_outputs = gen_pipe(prompt_messages, max_new_tokens=80)\n#         generated_prompt = prompt_outputs[0]['generated_text'][-1]['content']\n#         final_image_prompt = generated_prompt.strip().replace('\"', '')\n        \n#         with open(image_prompt_path, \"w\", encoding=\"utf-8\") as f:\n#             f.write(final_image_prompt) \n\n#         print(f\"image prompt saved to {image_prompt_path}\")\n\n#         return final_title, final_image_prompt\n#     finally:\n#         print(\"Cleaning up title gen model from memory\")\n#         del gen_model\n#         del gen_tokenizer\n#         del gen_pipe\n#         clear_memory()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-02T06:37:05.827737Z","iopub.execute_input":"2025-10-02T06:37:05.828029Z","iopub.status.idle":"2025-10-02T06:37:05.832913Z","shell.execute_reply.started":"2025-10-02T06:37:05.828007Z","shell.execute_reply":"2025-10-02T06:37:05.832098Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"def generate_newsletter_image(image_prompt_path = \"image_prompt.txt\", title_path = \"title.txt\", image_path = \"newsletter_image.png\"):\n    print(\"\\nStep 5 initialized, Generating image for newsletter\")\n\n    pipe = None\n    \n    try:\n        with open(image_prompt_path, 'r', encoding=\"utf-8\") as f:\n            visual_prompt = f.read().strip()\n        with open(title_path, 'r', encoding=\"utf-8\") as f:\n            title = f.read().strip()\n\n        final_prompt = f\"Image for a newsletter titled '{title}'. A clean, professional illustration of: {visual_prompt}. Marketing aesthetic, vibrant colors.\"\n        \n        pipe = DiffusionPipeline.from_pretrained(\n            path_sd,\n            torch_dtype = torch.float16,\n            variant = \"fp16\",\n            use_safetensors = True\n        )\n\n        pipe = pipe.to(device)\n\n        image = pipe(prompt = final_prompt).images[0]\n        image.save(image_path)\n        print(f\"Image saved to {image_path}\")\n\n        return image_path\n\n    finally:\n        print(\"Cleaning up diffusion model from memory\")\n        \n        if 'pipe' in locals() and pipe is not None:\n            del pipe\n\n        clear_memory()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-02T06:37:07.748363Z","iopub.execute_input":"2025-10-02T06:37:07.748622Z","iopub.status.idle":"2025-10-02T06:37:07.754723Z","shell.execute_reply.started":"2025-10-02T06:37:07.748603Z","shell.execute_reply":"2025-10-02T06:37:07.753886Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"def assemble(summary_path, image_path, title_path, output_name=\"newsletter\"):\n    print(\"Step 6 initialized, Assembling final Newsletter\")\n\n    try:\n        with open(summary_path, 'r', encoding=\"utf-8\") as f:\n            summary_text = f.read()\n        with open(title_path, 'r', encoding=\"utf-8\") as f:\n            title = f.read().strip()\n\n        mid = len(summary_text) // 2\n        split_point = summary_text.find(' ', mid)\n\n        if split_point == -1:\n            split_point = mid\n\n        summary_left = summary_text[:split_point]\n        summary_right = summary_text[split_point:].strip()\n\n        template = Environment().from_string(HTML_TEMPLATE_STRING)\n\n        html_content = template.render(\n            title = title,\n            summary_left = summary_left,\n            summary_right = summary_right,\n            image_file = image_path\n        )\n\n        html_file_path = f\"{output_name}.html\"\n\n        with open(html_file_path, 'w', encoding=\"utf-8\") as f:\n            f.write(html_content)\n\n        print(f\"HTML Newsletter saved to {html_file_path}\")\n\n        return html_file_path\n        \n    except Exception as e:\n        print(f\"An error occured during assembly= {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-02T06:37:09.144167Z","iopub.execute_input":"2025-10-02T06:37:09.144663Z","iopub.status.idle":"2025-10-02T06:37:09.150496Z","shell.execute_reply.started":"2025-10-02T06:37:09.144637Z","shell.execute_reply":"2025-10-02T06:37:09.149736Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"def run_full_pipeline(urls_string):\n    print(\"Starting automated Newsletter Generation\")\n\n    urls = [url.strip() for url in urls_string.split(',') if url.strip()]\n    if not urls:\n        yield \"Please upload atleast one url\", None\n        return\n    \n    content_file = \"content.txt\"\n    summary_file = \"summary.txt\"\n    title_file = \"title.txt\"\n    image_prompt_file = \"image_prompt.txt\"\n    image_file = \"newsletter_image.png\"\n\n    log = \"\"\n    try:\n        log += \"Step 1/4= Scraping content from URLs+++++\\n\"\n        yield log, None\n        scrape_from_web(urls, content_file)\n        log += \"Step 1/4= Scraping Completed\\n\"\n        yield log, None\n\n        log += \"Step 2/4= Summarizing, Generating Title+Image Prompt Content+++++\\n\"\n        yield log, None\n        summarize(content_file, summary_file, title_file, image_prompt_file)\n        log += \"Step 2/4= Summarizing, Generating Title+Image Prompt Complete\\n\"\n        yield log, None\n\n        # yield \"Step 3/5= Generating Title and Image_prompt+++++\", None\n        # generate_title_and_prompt(summary_file, title_file, image_prompt_file)\n        # yield \"Step 3/5= Title and Image_prompt Generated\", None\n        \n        log += \"Step 3/4= Generating Image++++\\n\"\n        yield log, None\n        image_file_path = generate_newsletter_image(image_prompt_file, title_file, image_file)\n        log += \"Step 3/4= Image Generated\\n\"\n        yield log, None\n\n        \n        log += \"Step 4/4= Assembling final Newsletter+++++\\n\"\n        yield log, None\n        html_file_path = assemble(summary_file, image_file_path, title_file)\n        log += \"Step 4/4= Newsletter Assembled\\n\"\n        yield log, None\n\n\n        log += \"Pipeline finished!!! Your newsletter is ready for download.\"    \n\n        downloadables = [content_file, summary_file, title_file, image_prompt_file, image_file_path, html_file_path]\n\n        yield log, downloadables\n        \n    except Exception as e:\n        error_message = f\"Error occurred = {str(e)}\"\n        print(error_message)\n        yield error_message, None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-02T06:37:10.462338Z","iopub.execute_input":"2025-10-02T06:37:10.462598Z","iopub.status.idle":"2025-10-02T06:37:10.469485Z","shell.execute_reply.started":"2025-10-02T06:37:10.462578Z","shell.execute_reply":"2025-10-02T06:37:10.468720Z"}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"with gr.Blocks(theme='monochrome', title=\"Agentic Newsletter Generator\") as demo:\n    gr.Markdown('''<div style=\"position: absolute; top: 20px; right: 20px; opacity: 0.9; z-index: -1;\">\n  <img src=\"https://ibb.co/jkfszwYT\" alt=\"Watermark\" width=\"200\">\n</div>\n\n<center>\n<h1>\nAgentic Newsletter Generator\n</h1>\n</center>\n\n- Enter one or more URLs (comma+separated)\n- The agent will automatically parse, summarize, and newsletter will be created\n---------\n''')\n    \n    with gr.Row():\n        with gr.Column(scale = 2):\n            url_input = gr.Textbox(label=\"Enter URLs Here\", placeholder=\"https://www.<websitename>.<domain>/\")\n            generate_button = gr.Button(\"Generate Newsletter\", variant=\"primary\")\n\n        with gr.Column(scale = 1):\n            status_output = gr.Textbox(label=\"Agent's progress\", interactive=False, lines=8)\n            file_output = gr.File(label=\"Downloadables output\")\n\n    generate_button.click(\n        fn = run_full_pipeline,\n        inputs = url_input,\n        outputs=[status_output, file_output]\n    )\n\nif __name__ == \"__main__\":\n    demo.launch(debug=True, share=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-02T06:37:11.837772Z","iopub.execute_input":"2025-10-02T06:37:11.838058Z"}},"outputs":[{"name":"stdout","text":"* Running on local URL:  http://127.0.0.1:7860\n* Running on public URL: https://26e9eff7d82aefea69.gradio.live\n\nThis share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<div><iframe src=\"https://26e9eff7d82aefea69.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"},"metadata":{}},{"name":"stdout","text":"Starting automated Newsletter Generation\nStep 1 initialized, Scraping content from the links\nSuccessfully Scraped content from = https://www.cnet.com/tech/computing/apple-ipad-pro-vision-pro-2-rumors-m5-chip-on-deck/\nContent saved to content.txt\n\nStep 2 initialized, Summarizing content\n","output_type":"stream"},{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\nThe `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"89eafe54aa75400d94817a2cb2bfa242"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n","output_type":"stream"},{"name":"stdout","text":"Full Summary saved to summary.txt\n\nStep 3 initialized, Generating Title\nGenerated title saved to title.txt\n\nStep 4 initialized, Generating Image prompt\nGenerated prompt saved to image_prompt.txt\nCleaning up Qwen model from memory\n\nStep 5 initialized, Generating image for newsletter\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a1cb1bafcb254711884d47be35431f69"}},"metadata":{}},{"name":"stderr","text":"Token indices sequence length is longer than the specified maximum sequence length for this model (97 > 77). Running this sequence through the model will result in indexing errors\nThe following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [\"of a modern office filled with smart gadgets, reflecting apple's continuous. marketing aesthetic, vibrant colors.\"]\nToken indices sequence length is longer than the specified maximum sequence length for this model (97 > 77). Running this sequence through the model will result in indexing errors\nThe following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [\"of a modern office filled with smart gadgets, reflecting apple's continuous. marketing aesthetic, vibrant colors.\"]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/50 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"635087a489174ed5b0e1fb3fce87ac34"}},"metadata":{}},{"name":"stdout","text":"Image saved to newsletter_image.png\nCleaning up diffusion model from memory\nStep 6 initialized, Assembling final Newsletter\nHTML Newsletter saved to newsletter.html\nStarting automated Newsletter Generation\nStep 1 initialized, Scraping content from the links\nSuccessfully Scraped content from = https://www.cnet.com/tech/computing/apple-ipad-pro-vision-pro-2-rumors-m5-chip-on-deck/\nContent saved to content.txt\n\nStep 2 initialized, Summarizing content\n","output_type":"stream"},{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\nThe `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3a58537b7aec4ac99547e4c272958aec"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}